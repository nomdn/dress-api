import asyncio
import sys
from tqdm import tqdm
import os
from pathlib import Path
import subprocess
import random
import json
import logging
import httpx
import colorama
from datetime import datetime
import logging
from typing import List, Tuple, Union, Optional, Dict
from colorama import Fore, Style
import uvicorn
from dotenv import load_dotenv
from git import Repo

# é…ç½®æ—¥å¿—


def normalize_url(path: str) -> str:
    """
    å°†æ–‡ä»¶è·¯å¾„ä¸­çš„ '#' æ›¿æ¢ä¸º URL å®‰å…¨çš„ '%23'
    æ³¨æ„ï¼šè¾“å…¥åº”ä¸ºç›¸å¯¹è·¯å¾„å­—ç¬¦ä¸²ï¼ˆå¦‚ "#/a.jpg"ï¼‰
    """
    return path.replace("#", "%23")


async def _run_git_log_follow(repo: Repo, file_path: str) -> List[List[str]]:
    """
    æ‰§è¡Œ git log --follow --format="%H|%an|%ae|%cI" -- <file>
    ä½¿ç”¨ repo.working_dir ä½œä¸º cwd
    è¿”å› [[commit_hash, author_name, author_email, committed_iso_time], ...]
    """
    try:
        # ä¿®å¤ï¼šä½¿ç”¨ subprocess.run è€Œä¸æ˜¯ asyncio.subprocess.run
        result = subprocess.run(
            [
                "git", "log", "--follow",
                "--format=%H|%an|%ae|%cI",
                "--", file_path
            ],
            cwd=repo.working_dir,  # ğŸ‘ˆ å…³é”®ï¼šä» repo å¯¹è±¡è·å–è·¯å¾„
            capture_output=True,
            text=True,
            encoding='utf-8',
            timeout=30
        )
        if result.returncode != 0:
            logging.warning(f"git log --follow failed for {file_path}: {result.stderr}")
            return []
        
        lines = []
        for line in result.stdout.strip().split('\n'):
            if line and '|' in line:
                parts = line.split('|', 3)
                if len(parts) == 4:
                    lines.append(parts)
        return lines
    except Exception as e:
        logging.error(f"æ‰§è¡Œ git log --follow å‡ºé”™ ({file_path}): {e}")
        return []

async def get_all_committers(repo: Repo, file_path: str) -> Tuple[List[Tuple[str, str]], Optional[datetime]]:
    """è·å–æŒ‡å®šæ–‡ä»¶æ‰€æœ‰å†å²æäº¤çš„ä½œè€…ï¼ˆå»é‡ï¼‰ï¼Œä½¿ç”¨ --follow è¿½è¸ªé‡å‘½å"""
    commits = await _run_git_log_follow(repo, file_path)
    seen = set()
    authors = []
    for _, author_name, author_email, _ in commits:
        if (author_name, author_email) not in seen:
            seen.add((author_name, author_email))
            authors.append((author_name, author_email))
    # è·å–æœ€æ–°æäº¤æ—¶é—´
    latest_time = None
    if commits:
        iso_time = commits[0][3]  # æœ€æ–° commit
        try:
            latest_time = datetime.fromisoformat(iso_time.replace('Z', '+00:00'))
        except ValueError as e:
            logging.warning(f"æ—¶é—´è§£æå¤±è´¥ ({iso_time}): {e}")
            latest_time = None

    return list(authors), latest_time

async def get_commit_time(repo: Repo, file_path: str) -> Optional[datetime]:
    """è·å–æŒ‡å®šæ–‡ä»¶æœ€æ–°æäº¤æ—¶é—´"""
    commits = await _run_git_log_follow(repo, file_path)
    if commits:
        iso_time = commits[0][3]  # æœ€æ–° commit
        try:
            return datetime.fromisoformat(iso_time.replace('Z', '+00:00'))
        except ValueError as e:
            logging.warning(f"æ—¶é—´è§£æå¤±è´¥ ({iso_time}): {e}")
    return None
async def get_first_commit_author(repo: Repo, file_path: str) -> Optional[Tuple[str, str]]:
    """è·å–æ–‡ä»¶é¦–æ¬¡æ·»åŠ çš„ä½œè€…ï¼ˆç”¨äºè´¡çŒ®ç»Ÿè®¡ï¼‰"""
    commits = await _run_git_log_follow(repo, file_path)
    if commits:
        first_commit = commits[-1]  # æœ€æ—©çš„ commit
        return (first_commit[1], first_commit[2])
    return None
async def get_github_index(index:str="index_0.json") -> Dict:
    """è·å–è¿œç«¯ GitHub ç´¢å¼•æ•°æ®"""
    try:
        async with httpx.AsyncClient() as client:
            response = await client.get(
                url=f"https://cdn.jsdelivr.net/gh/nomdn/dress-api@main/public/{index}",
                timeout=10.0
            )
        response.raise_for_status()
        data = response.json()
        return data
    except (httpx.TimeoutException, httpx.RequestError, httpx.HTTPStatusError):
        logging.warning("è·å–è¿œç«¯ç´¢å¼•æ•°æ®è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•...")
        # ä¿®æ­£ CDN åŸŸåæ‹¼å†™ï¼ˆjsdelivr.netï¼Œä¸æ˜¯ jsdeliver.netï¼‰
        for i in [
            "https://cdn.jsdelivr.net/",
            "https://fastly.jsdelivr.net/",
            "https://gcore.jsdelivr.net/",
            "https://testingcf.jsdelivr.net/"
        ]:
            try:
                async with httpx.AsyncClient() as client:
                    response = await client.get(
                        url=f"{i}gh/nomdn/dress-api@main/public/{index}",
                        timeout=10.0
                    )
                response.raise_for_status()
                data = response.json()
                return data
            except (httpx.TimeoutException, httpx.RequestError, httpx.HTTPStatusError):
                continue
        else:
            raise RuntimeError("è·å–è¿œç«¯æ•°æ®å¤±è´¥ï¼")

def run_git_pull():
    """åœ¨åå°æ‰§è¡Œ git pull"""
    try:
        result = subprocess.run(
            ["git", "pull"],
            cwd="Dress",  # ğŸ‘ˆ æ›¿æ¢ä¸ºä½ çš„æœ¬åœ°ä»“åº“è·¯å¾„
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode != 0:
            logging.error(f"Git pull failed: {result.stderr}")
        else:
            logging.info("Git pull succeeded")
    except subprocess.TimeoutExpired as e:
        logging.error(f"Git pull è¶…æ—¶: {e}")
    except subprocess.SubprocessError as e:
        logging.error(f"Git pull å­è¿›ç¨‹é”™è¯¯: {e}")
    except Exception as e:
        logging.error(f"Git pull æœªçŸ¥é”™è¯¯: {e}")

def get_dress_image_paths(IMG_EXTENSIONS: set = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}) -> List[str]:
    # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆå³ä¸»ç¨‹åºç›®å½•ï¼‰
    main_dir = Path(__file__).parent.resolve()

    # Dress ç›®å½•è·¯å¾„ï¼ˆä¸»ç¨‹åºç›®å½•ä¸‹çš„å­ç›®å½•ï¼‰
    dress_dir = main_dir / "Dress"

    if not dress_dir.exists():
        raise FileNotFoundError(f"Dress ç›®å½•ä¸å­˜åœ¨: {dress_dir}")

    image_paths = []

    # é€’å½’éå† Dress ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
    for file_path in dress_dir.rglob('*'):
        if file_path.is_file() and file_path.suffix.lower() in IMG_EXTENSIONS:
            real_path = file_path.relative_to(dress_dir)
            # å¼ºåˆ¶è½¬æ¢ä¸º POSIX é£æ ¼ï¼ˆ/ åˆ†éš”ï¼‰ï¼Œæ— è®ºæ“ä½œç³»ç»Ÿ
            posix_path = real_path.as_posix()  # ğŸ‘ˆ å…³é”®ï¼
            image_paths.append(posix_path)

    return sorted(image_paths)

async def build_index(repo: Repo) -> Dict[int, List]:
    """
    æ„å»ºå›¾ç‰‡ç´¢å¼•å­—å…¸ï¼Œé”®ä¸ºåºå·ï¼Œå€¼ä¸º [ç›¸å¯¹è·¯å¾„, æäº¤è€…åˆ—è¡¨, æœ€æ–°æäº¤æ—¶é—´]
    
    Args:
        repo (Repo): Git ä»“åº“å¯¹è±¡

    Returns:
        Dict[int, List]: ç´¢å¼•å­—å…¸

    Raises:
        FileNotFoundError: å½“Dressç›®å½•ä¸å­˜åœ¨æ—¶
        PermissionError: å½“æ²¡æœ‰è¶³å¤Ÿæƒé™è®¿é—®æ–‡ä»¶æ—¶
        Exception: å…¶ä»–æœªé¢„æœŸçš„é”™è¯¯
    """
    index = {}

    try:
        paths = get_dress_image_paths()
        logging.info(f"å…±æ‰¾åˆ° {len(paths)} å¼ å›¾ç‰‡")

        for c, i in enumerate(tqdm(paths, desc="æ„å»ºç´¢å¼•",file=sys.stdout), start=1):
            uploader_data,latest_commit_time = await get_all_committers(repo, i)
            if not uploader_data:
                logging.warning(f"âš ï¸ è­¦å‘Š: {i} æ— æäº¤è®°å½•ï¼Œè·³è¿‡")
                continue
            logging.debug(f"å¤„ç†å›¾ç‰‡ {c}: {i}, ä¸Šä¼ è€…: {uploader_data}, æœ€æ–°æäº¤æ—¶é—´: {latest_commit_time}")
            # åŒ…å«æ—¶é—´ä¿¡æ¯
            index[c] = [i, uploader_data, latest_commit_time]

        return index

    except FileNotFoundError:
        raise
    except PermissionError:
        raise
    except Exception as e:
        logging.error(f"æ„å»ºç´¢å¼•æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        raise


async def build_index_by_author(repo: Repo) -> Dict[str, List[Dict]]:
    """
    æ„å»ºæŒ‰**é¦–æ¬¡æäº¤ä½œè€…**åˆ†ç»„çš„å›¾ç‰‡ç´¢å¼•
    """
    index_name = {}
    paths = get_dress_image_paths()
    logging.info(f"å…±æ‰¾åˆ° {len(paths)} å¼ å›¾ç‰‡")

    for i in paths:
        first_author = await get_first_commit_author(repo, i)  # ğŸ‘ˆ ä½¿ç”¨æ–°å‡½æ•°
        latest_time = await get_commit_time(repo, i)
        
        if not first_author:
            logging.warning(f"âš ï¸ è­¦å‘Š: {i} æ— æ³•è¿½è¸ªé¦–æ¬¡ä½œè€…ï¼Œè·³è¿‡")
            continue
            
        author_name = first_author[0]
        if author_name not in index_name:
            index_name[author_name] = []
        index_name[author_name].append({
            "path": i,
            "latest_commit_time": latest_time
        })
        logging.debug(f"å½’å±: {i} â†’ {author_name}")

    return index_name

async def convert_index_id_to_index_author(index_0: dict) -> dict:
    """
    ä» index_0 æ„å»º index_1ï¼ˆæŒ‰é¦–æ¬¡ä½œè€…åˆ†ç»„ï¼‰
    é¦–æ¬¡ä½œè€… = uploader_list[-1]ï¼ˆå› ä¸º build_index ä¸­å»é‡é¡ºåºæ˜¯â€œæœ€æ–°â†’æœ€æ—©â€ï¼‰
    """
    index_1 = {}
    for entry in index_0.values():
        path = entry[0]
        uploader_list = entry[1]      # [(name, email), ...]ï¼Œæœ€æ–°åœ¨å‰ï¼Œæœ€æ—©åœ¨å
        latest_time = entry[2]
        first_author_name = uploader_list[-1][0]  # ğŸ‘ˆ é¦–æ¬¡ä½œè€…ä¸€èˆ¬æ˜¯åˆ—è¡¨æœ€åä¸€ä¸ª
        if not uploader_list:
            continue

        # é¦–æ¬¡ä½œè€…æ˜¯åˆ—è¡¨æœ€åä¸€ä¸ª
        for name in uploader_list:
            if name[0] =="CuteDress":
                logging.debug(f"å‘ç° CuteDress ä½œä¸ºæäº¤è€…ï¼Œè·¯å¾„: {path}")  
            else:
                first_author_name = name[0]
                logging.debug(f"å·²è¿½è¸ªåˆ°æ­£ç¡®ä½œè€…: {first_author_name}ï¼Œè·¯å¾„: {path}")
        if first_author_name not in index_1:
            index_1[first_author_name] = []
        index_1[first_author_name].append({
            "path": path,
            "latest_commit_time": latest_time
        })
    return index_1
def escape_hash_in_index(index_data: Dict, index_type: str) -> Dict:
    """
    å°†è·¯å¾„ä¸­çš„ '#' æ›¿æ¢ä¸º '%23'
    - index_type="url":   å¤„ç† index_0 {id: [path, uploader, latest_commit_time]}
    - index_type="author": å¤„ç† index_1 {author: [{"path": path, "latest_commit_time": time}, ...]}
    """
    if not isinstance(index_data, dict):
        raise TypeError("è¾“å…¥å¿…é¡»æ˜¯å­—å…¸")
    
    result = {}
    
    if index_type == "url":
        for key, value in index_data.items():
            if isinstance(value, list) and len(value) >= 1:
                path = normalize_url(value[0])  # å¤„ç†è·¯å¾„ä¸­çš„#å­—ç¬¦
                uploader_data = value[1]  # æäº¤è€…åˆ—è¡¨
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—¶é—´ä¿¡æ¯
                if len(value) > 2:
                    latest_commit_time = value[2]
                    # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œè½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
                    if hasattr(latest_commit_time, 'isoformat'):
                        latest_commit_time_str = latest_commit_time.isoformat()
                    else:
                        latest_commit_time_str = latest_commit_time
                    
                    result[key] = [path, uploader_data, latest_commit_time_str]
                else:
                    result[key] = [path, uploader_data]  # ä¿æŒåŸæœ‰çš„ç»“æ„
                
    elif index_type == "author":
        for author, items in index_data.items():
            if isinstance(items, list):
                processed_items = []
                for item in items:
                    if isinstance(item, dict) and "path" in item:
                        # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œå¤„ç†å…¶ä¸­çš„pathå­—æ®µï¼Œå¹¶å°†datetimeè½¬ä¸ºå­—ç¬¦ä¸²
                        normalized_path = normalize_url(item["path"])
                        latest_commit_time = item.get("latest_commit_time")
                        
                        # å¦‚æœæ˜¯datetimeå¯¹è±¡ï¼Œè½¬æ¢ä¸ºISOæ ¼å¼å­—ç¬¦ä¸²
                        if hasattr(latest_commit_time, 'isoformat'):
                            latest_commit_time_str = latest_commit_time.isoformat()
                        else:
                            latest_commit_time_str = latest_commit_time
                        
                        processed_item = {
                            "path": normalized_path,
                            "latest_commit_time": latest_commit_time_str
                        }
                        processed_items.append(processed_item)
                    elif isinstance(item, str):
                        # å¦‚æœæ˜¯å­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥å¤„ç†
                        processed_items.append(normalize_url(item))
                    else:
                        # å…¶ä»–æƒ…å†µä¿æŒåŸæ ·
                        processed_items.append(item)
                result[author] = processed_items
                
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç±»å‹: {index_type}")
        
    return result